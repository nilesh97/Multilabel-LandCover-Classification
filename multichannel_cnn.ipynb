{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project_ucm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYGM0YemzksD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "d65ba0c1-1fe2-47dd-9a78-90d617c0b733"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.io as scio\n",
        "import scipy.ndimage as im\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import applications\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model, Model\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, RepeatVector\n",
        "from keras.layers import GlobalAveragePooling2D, BatchNormalization, ZeroPadding2D, UpSampling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Reshape, Add, Multiply, Lambda, AveragePooling2D\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import MaxPooling2D, Dropout, Input, MaxPool2D\n",
        "from keras.optimizers import SGD, Adam, Nadam, RMSprop, Adagrad\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import hamming_loss\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.activations import linear as linear_activation\n",
        "from keras import initializers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from  sklearn.metrics  import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-H9ZxbKzucn",
        "colab_type": "code",
        "outputId": "369ff6da-ed65-405d-a4e6-8bbd04717fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/nilesh97/Multilabel-LandCover-Classification.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Multilabel-LandCover-Classification'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/33)\u001b[K\rremote: Counting objects:   6% (2/33)\u001b[K\rremote: Counting objects:   9% (3/33)\u001b[K\rremote: Counting objects:  12% (4/33)\u001b[K\rremote: Counting objects:  15% (5/33)\u001b[K\rremote: Counting objects:  18% (6/33)\u001b[K\rremote: Counting objects:  21% (7/33)\u001b[K\rremote: Counting objects:  24% (8/33)\u001b[K\rremote: Counting objects:  27% (9/33)\u001b[K\rremote: Counting objects:  30% (10/33)\u001b[K\rremote: Counting objects:  33% (11/33)\u001b[K\rremote: Counting objects:  36% (12/33)\u001b[K\rremote: Counting objects:  39% (13/33)\u001b[K\rremote: Counting objects:  42% (14/33)\u001b[K\rremote: Counting objects:  45% (15/33)\u001b[K\rremote: Counting objects:  48% (16/33)\u001b[K\rremote: Counting objects:  51% (17/33)\u001b[K\rremote: Counting objects:  54% (18/33)\u001b[K\rremote: Counting objects:  57% (19/33)\u001b[K\rremote: Counting objects:  60% (20/33)\u001b[K\rremote: Counting objects:  63% (21/33)\u001b[K\rremote: Counting objects:  66% (22/33)\u001b[K\rremote: Counting objects:  69% (23/33)\u001b[K\rremote: Counting objects:  72% (24/33)\u001b[K\rremote: Counting objects:  75% (25/33)\u001b[K\rremote: Counting objects:  78% (26/33)\u001b[K\rremote: Counting objects:  81% (27/33)\u001b[K\rremote: Counting objects:  84% (28/33)\rremote: Counting objects:  87% (29/33)\u001b[K\rremote: Counting objects:  90% (30/33)\u001b[K\rremote: Counting objects:  93% (31/33)\u001b[K\rremote: Counting objects:  96% (32/33)\u001b[K\rremote: Counting objects: 100% (33/33)\u001b[K\rremote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 5230 (delta 0), reused 32 (delta 0), pack-reused 5197\u001b[K\n",
            "Receiving objects: 100% (5230/5230), 984.98 MiB | 45.02 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "Checking out files: 100% (8106/8106), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkpT8Dch0jSK",
        "colab_type": "code",
        "outputId": "9928a1f4-1d1d-4c33-a8d7-e7e0871b1988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Load the labels\n",
        "path_to_labels = 'Multilabel-LandCover-Classification/UCMerced_LandUse_Multilabels/LandUse_multilabels.mat' # CHANGE THIS PATH ACCORDINGLY\n",
        "\n",
        "mat_labels   = scio.loadmat(path_to_labels)\n",
        "numpy_labels = np.transpose( mat_labels['labels'] )\n",
        "numpy_labels = np.array(numpy_labels, dtype=np.float32)\n",
        "\n",
        "print( \"The shape of the label matrix is {}.\\n\".format(numpy_labels.shape) )\n",
        "\n",
        "\n",
        "# Load the images.\n",
        "path_to_images = 'Multilabel-LandCover-Classification/UCMerced_LandUse_Images/'  # CHANGE THIS PATH ACCORDINGLY\n",
        "\n",
        "image_types = [ 'agricultural',  'airplane',    'baseballdiamond', 'beach',   'buildings',          'chaparral',         'denseresidential',\n",
        "                'forest',        'freeway',     'golfcourse',      'harbor',  'intersection',       'mediumresidential', 'mobilehomepark',\n",
        "                'overpass',      'parkinglot',  'river',           'runway',  'sparseresidential',  'storagetanks',      'tenniscourt' ]\n",
        "\n",
        "extension = '.tif'\n",
        "\n",
        "\n",
        "images = []\n",
        "\n",
        "# In reality not all images are the same size. So we will crop them in the smallest dimensions possible.\n",
        "x_limit = 256\n",
        "y_limit = 256\n",
        "\n",
        "\n",
        "for current_type in image_types:\n",
        "\n",
        "    for index in range(100):\n",
        "\n",
        "        str_index = str(index)\n",
        "\n",
        "        if index < 10:\n",
        "            str_index = '0' + str_index\n",
        "\n",
        "        current_image = imageio.imread(path_to_images + current_type + '/' + current_type + str_index + extension)\n",
        "\n",
        "        # Crop the images.\n",
        "        if current_image.shape[0] > 256:\n",
        "          remove_from_x = current_image.shape[0] - x_limit\n",
        "          remove_from_y = current_image.shape[1] - y_limit\n",
        "          current_image = current_image[remove_from_x:, remove_from_y:]\n",
        "        \n",
        "        if current_image.shape[0] != 256:\n",
        "          pad_x = 256 - current_image.shape[0]\n",
        "          z = np.zeros((pad_x,current_image.shape[1],3), dtype=np.int64)\n",
        "          current_image = np.concatenate((current_image,z),axis=0)\n",
        "        \n",
        "        if current_image.shape[1] != 256:\n",
        "          pad_y = 256 - current_image.shape[1]\n",
        "          p = np.zeros((256,pad_y,3), dtype=np.int64)\n",
        "          current_image = np.concatenate((current_image,p),axis=1)\n",
        "          \n",
        "        images.append(current_image)\n",
        "\n",
        "\n",
        "print (len (images), len(images[0]), len (images[0][0]), len(images[0][0][0]))\n",
        "images = np.array(images, dtype=np.float32)\n",
        "images = images / 255\n",
        "print(\"hi\")\n",
        "\n",
        "print( \"The shape of the image samples matrix is {}.\\n\".format(images.shape) )\n",
        "\n",
        "\n",
        "# Shuffle the data.\n",
        "random_indices = np.arange( images.shape[0] )\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(random_indices)\n",
        "\n",
        "numpy_labels     = numpy_labels[random_indices]\n",
        "images           = images[random_indices]\n",
        "\n",
        "\n",
        "# Pickle the data.\n",
        "np.save(\"UcmImages.npy\", images)\n",
        "np.save(\"UcmLabels.npy\", numpy_labels)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the label matrix is (2100, 17).\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'decode_packbits'\n",
            "  Functionality might be degraded or be slow.\n",
            "\n",
            "  warnings.warn(\"%s%s\" % (e, warn))\n",
            "/usr/local/lib/python3.6/dist-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'decode_lzw'\n",
            "  Functionality might be degraded or be slow.\n",
            "\n",
            "  warnings.warn(\"%s%s\" % (e, warn))\n",
            "/usr/local/lib/python3.6/dist-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'unpack_ints'\n",
            "  Functionality might be degraded or be slow.\n",
            "\n",
            "  warnings.warn(\"%s%s\" % (e, warn))\n",
            "/usr/local/lib/python3.6/dist-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'reverse_bitorder'\n",
            "  Functionality might be degraded or be slow.\n",
            "\n",
            "  warnings.warn(\"%s%s\" % (e, warn))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2100 256 256 3\n",
            "hi\n",
            "The shape of the image samples matrix is (2100, 256, 256, 3).\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTCxBsNN4z1f",
        "colab_type": "code",
        "outputId": "6af0bc7a-b524-4c54-ee2c-a760d08327cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(random_indices)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1034 1176   67 ... 1130 1294  860]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "450bIXlP0rmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the data into training and test sets\n",
        "OBSERVATIONS_FILE  = 'UcmImages.npy' # The file containing the data samples.\n",
        "LABELS_FILE        = 'UcmLabels.npy' # The file containing the labels.\n",
        "TESTING_DATA_NUM = 400\n",
        "\n",
        "images = np.load(OBSERVATIONS_FILE)\n",
        "labels = np.load(LABELS_FILE)\n",
        "\n",
        "random_indices = np.arange( images.shape[0] )\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(random_indices)\n",
        "\n",
        "labels = labels[random_indices]\n",
        "images = images[random_indices]\n",
        "\n",
        "test_set    = images[:TESTING_DATA_NUM]\n",
        "test_labels = labels[:TESTING_DATA_NUM]\n",
        "\n",
        "train_set = images[TESTING_DATA_NUM:]\n",
        "train_labels = labels[TESTING_DATA_NUM:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U2WrT1g5PUD",
        "colab_type": "code",
        "outputId": "7ced735c-96b3-401f-9903-dda6b6f8eb88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(random_indices)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1034 1176   67 ... 1130 1294  860]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNfLDNJ80ziF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parameters For Data Augmentation later\n",
        "ROTATION_RANGE  = 45\n",
        "SHIFT_FRACTION  = 0.2\n",
        "SHEAR_RANGE     = 0.0\n",
        "ZOOM_RANGE      = 0.0\n",
        "HORIZONTAL_FLIP = True\n",
        "VERTICAL_FILP   = True\n",
        "\n",
        "data_generator = ImageDataGenerator( \n",
        "                                         rotation_range=ROTATION_RANGE, width_shift_range=SHIFT_FRACTION, height_shift_range=SHIFT_FRACTION,\n",
        "                                         shear_range=SHEAR_RANGE,       zoom_range=ZOOM_RANGE,            horizontal_flip=HORIZONTAL_FLIP,\n",
        "                                         vertical_flip=VERTICAL_FILP)\n",
        "data_generator.fit(train_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS85Ah3u1CHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Useful Callbacks:\n",
        "def CALLBACKS():\n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, min_lr=10e-8, epsilon=0.01, verbose=1)\n",
        "  early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
        "  callbacks= [lr_reducer, early_stopper]\n",
        "  \n",
        "  return callbacks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElmpkKdf1E9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Accuracies:\n",
        "def findMetrics(yTrue, yPred):\n",
        "  # precision overall\n",
        "  positive_predictions = np.count_nonzero(yPred) # denominator\n",
        "  true_positives       = np.sum( np.logical_and(yTrue == 1, yPred == 1) ) # numerator\n",
        "\n",
        "  if positive_predictions == 0:\n",
        "      precision = 0\n",
        "  else:\n",
        "      precision = true_positives / positive_predictions\n",
        "\n",
        "\n",
        "  # recall overall\n",
        "  relevant_positives = np.count_nonzero(yTrue) # denominator\n",
        "\n",
        "  recall = true_positives / relevant_positives\n",
        "\n",
        "\n",
        "  # F Measure overall\n",
        "  numerator   = precision * recall\n",
        "  denominator = precision + recall\n",
        "\n",
        "  if denominator == 0:\n",
        "      f_measure = 0\n",
        "  else:\n",
        "      f_measure = (2 * numerator) / denominator\n",
        "  \n",
        "  # precision per row/column\n",
        "  positive_predictions_row = np.count_nonzero(yPred, axis=1) # denominators\n",
        "  positive_predictions_col = np.count_nonzero(yPred, axis=0) # denominators\n",
        "\n",
        "  true_positives_row       = np.sum( np.logical_and(yTrue == 1, yPred == 1), axis=1 ) # numerators\n",
        "  true_positives_col       = np.sum( np.logical_and(yTrue == 1, yPred == 1), axis=0 ) # numerators\n",
        "\n",
        "  positive_predictions_row = positive_predictions_row.astype('float')\n",
        "  positive_predictions_col = positive_predictions_col.astype('float')\n",
        "\n",
        "  true_positives_row       = true_positives_row.astype('float')\n",
        "  true_positives_col       = true_positives_col.astype('float')\n",
        "\n",
        "  precision_per_row        = np.true_divide( true_positives_row, positive_predictions_row, out=np.zeros_like(true_positives_row), where=positive_predictions_row!=0 )\n",
        "  precision_per_col        = np.true_divide( true_positives_col, positive_predictions_col, out=np.zeros_like(true_positives_col), where=positive_predictions_col!=0 )\n",
        "\n",
        "  avrg_precision_row       = np.mean(precision_per_row)\n",
        "  avrg_precision_col       = np.mean(precision_per_col)\n",
        "\n",
        "  # multi_label accuracy overall\n",
        "  accuracy2 = true_positives / ( np.sum( np.logical_or(yTrue == 1, yPred == 1) ) )\n",
        "  \n",
        "  acc2_denominator_row = np.sum( np.logical_or(yTrue == 1, yPred == 1), axis=1 )\n",
        "  \n",
        "  acc2_denominator_row = acc2_denominator_row.astype('float')\n",
        "  \n",
        "  accuracy2_row        = np.true_divide( true_positives_row, acc2_denominator_row, out=np.zeros_like(true_positives_row), where=acc2_denominator_row!=0 )\n",
        "  \n",
        "  avrg_acc2_row        = np.mean(accuracy2_row)\n",
        "  \n",
        "  # recall per row/column\n",
        "  relevant_positives_row   = np.count_nonzero(yTrue, axis=1) # denominators\n",
        "  relevant_positives_col   = np.count_nonzero(yTrue, axis=0) # denominators\n",
        "\n",
        "  relevant_positives_row   = relevant_positives_row.astype('float')\n",
        "  relevant_positives_col   = relevant_positives_col.astype('float')\n",
        "\n",
        "  recall_per_row           = np.true_divide( true_positives_row, relevant_positives_row, out=np.zeros_like(true_positives_row), where=relevant_positives_row!=0 )\n",
        "  recall_per_col           = np.true_divide( true_positives_col, relevant_positives_col, out=np.zeros_like(true_positives_col), where=relevant_positives_col!=0 )\n",
        "\n",
        "  avrg_recall_row          = np.mean(recall_per_row)\n",
        "  avrg_recall_col          = np.mean(recall_per_col)\n",
        "\n",
        "\n",
        "  # F Measure per row\n",
        "  numerator_row   = avrg_precision_row * avrg_recall_row\n",
        "  denominator_row = avrg_precision_row + avrg_recall_row\n",
        "\n",
        "  if denominator_row == 0:\n",
        "      f1_measure_row = 0\n",
        "      f2_measure_row = 0\n",
        "  else:\n",
        "      f1_measure_row = (2 * numerator_row) / denominator_row\n",
        "      f2_measure_row = ((5 * numerator_row) / ((4 * avrg_precision_row) + (avrg_recall_row)))\n",
        "      \n",
        "  print(\"Accuracy is :: \" + str(avrg_acc2_row))\n",
        "  print(\"F1 Score is :: \" + str(f1_measure_row))\n",
        "  print(\"F2 Score is :: \" + str(f2_measure_row))\n",
        "  print(\"Precision row :: \" + str(avrg_precision_row))\n",
        "  print(\"Recall row :: \" + str(avrg_recall_row))\n",
        "  print(\"Precision column :: \" + str(avrg_precision_col))\n",
        "  print(\"Recall column :: \" + str(avrg_recall_col))\n",
        "  return accuracy2, precision, recall, f_measure, avrg_precision_row, avrg_recall_row, f1_measure_row, f2_measure_row, avrg_precision_col, avrg_recall_col, avrg_acc2_row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB3GuJxY1KWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#different threshold values\n",
        "def thresholding1(test_set, test_labels):\n",
        "  out = model1.predict(test_set)\n",
        "  out = np.array(out)\n",
        "  #threshold = np.arange(0.1,0.9,0.05)\n",
        "  #for t in threshold:\n",
        "  for i in range(1):\n",
        "    #Thresholding function\n",
        "    threshold = np.arange(0.1,0.9,0.01)\n",
        "\n",
        "    acc = []\n",
        "    accuracies = []\n",
        "    otsu = []\n",
        "\n",
        "    best_threshold = np.zeros(out.shape[1])\n",
        "    \n",
        "    for i in range(out.shape[1]):\n",
        "        y_prob = np.array(out[:,i])\n",
        "\n",
        "        for j in threshold:\n",
        "            y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
        "            #acc.append( matthews_corrcoef(test_labels[:,i],y_pred))\n",
        "            #acc.append( fbeta_score(test_labels[:,i],y_pred,beta=1))\n",
        "            acc.append( accuracy_score(test_labels[:,i],y_pred))\n",
        "\n",
        "        acc   = np.array(acc)\n",
        "        index = np.where(acc==acc.max()) \n",
        "        accuracies.append(acc.max()) \n",
        "        best_threshold[i] = threshold[index[0][0]]\n",
        "        acc = []\n",
        "    \n",
        "    #best_threshold=[0.45]*17\n",
        "    print(best_threshold)\n",
        "\n",
        "    y_pred = np.array([[1 if out[i,j]>=best_threshold[j] else 0 for j in range(test_labels.shape[1])] for i in range(len(test_labels))])\n",
        "\n",
        "    print(hamming_loss(test_labels,y_pred))\n",
        "\n",
        "    x = findMetrics(test_labels, y_pred)\n",
        "    print('  Classification Report:\\n',classification_report(test_labels,y_pred),'\\n')\n",
        "    print(x)\n",
        "\n",
        "#different threshold values\n",
        "def thresholding2(test_set, test_labels):\n",
        "  out = model2.predict(test_set)\n",
        "  out = np.array(out)\n",
        "  #threshold = np.arange(0.1,0.9,0.05)\n",
        "  #for t in threshold:\n",
        "  for i in range(1):\n",
        "    #Thresholding function\n",
        "    threshold = np.arange(0.1,0.9,0.01)\n",
        "\n",
        "    acc = []\n",
        "    accuracies = []\n",
        "    otsu = []\n",
        "\n",
        "    best_threshold = np.zeros(out.shape[1])\n",
        "    \n",
        "    for i in range(out.shape[1]):\n",
        "        y_prob = np.array(out[:,i])\n",
        "\n",
        "        for j in threshold:\n",
        "            y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
        "            #acc.append( matthews_corrcoef(test_labels[:,i],y_pred))\n",
        "            #acc.append( fbeta_score(test_labels[:,i],y_pred,beta=1))\n",
        "            acc.append( accuracy_score(test_labels[:,i],y_pred))\n",
        "\n",
        "        acc   = np.array(acc)\n",
        "        index = np.where(acc==acc.max()) \n",
        "        accuracies.append(acc.max()) \n",
        "        best_threshold[i] = threshold[index[0][0]]\n",
        "        acc = []\n",
        "    \n",
        "    #best_threshold=[0.45]*17\n",
        "    print(best_threshold)\n",
        "\n",
        "    y_pred = np.array([[1 if out[i,j]>=best_threshold[j] else 0 for j in range(test_labels.shape[1])] for i in range(len(test_labels))])\n",
        "\n",
        "    print(hamming_loss(test_labels,y_pred))\n",
        "\n",
        "    x = findMetrics(test_labels, y_pred)\n",
        "    print('  Classification Report:\\n',classification_report(test_labels,y_pred),'\\n')\n",
        "    print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwMG86Ao1SJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def VGGNET():\n",
        "  vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(247, 242, 3))\n",
        "  \n",
        "  for layers in vgg_model.layers:\n",
        "    layers.trainable = True\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(vgg_model)\n",
        "  #model.add(GlobalAveragePooling2D())\n",
        "  model.add(Flatten(name='flatten_1'))\n",
        "\n",
        "  model.add(Dense(17, activation='sigmoid', name='dense_1'))\n",
        "  \n",
        "  return model\n",
        "\n",
        "def CA_VGG_LSTM():\n",
        "  vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(247, 242, 3))\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  for layer in tuple(vgg_model.layers[:-5]):\n",
        "    layer_type = type(layer).__name__\n",
        "    model.add(layer)\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), activation='relu', name='block5_conv1'))\n",
        "  model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2'))\n",
        "  model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3'))\n",
        "\n",
        "  model.add(Conv2D(17, kernel_size=(1, 1), strides=(1, 1), kernel_initializer='glorot_uniform'))\n",
        "  model.add(Reshape((17, 28*28), input_shape=(28, 28, 17)))\n",
        "\n",
        "  model.add(LSTM(17, input_shape=(17, 28*28), activation='tanh', kernel_initializer=initializers.RandomUniform(minval=-0.1, maxval=0.1, seed=None)))\n",
        "  \n",
        "  model.add(Dense(17, activation='sigmoid'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def CA_VGG_BILSTM():\n",
        "  vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(247, 242, 3))\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  for layer in tuple(vgg_model.layers[:-5]):\n",
        "    layer_type = type(layer).__name__\n",
        "    model.add(layer)\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), activation='relu', name='block5_conv1'))\n",
        "  model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2'))\n",
        "  model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3'))\n",
        "\n",
        "  model.add(Conv2D(17, kernel_size=(1, 1), strides=(1, 1), kernel_initializer='glorot_uniform'))\n",
        "  model.add(Reshape((17, 28*28), input_shape=(28, 28, 17)))\n",
        "\n",
        "  model.add(Bidirectional(LSTM(17, input_shape=(17, 28*28), activation='tanh', kernel_initializer=initializers.RandomUniform(minval=-0.1, maxval=0.1, seed=None)), merge_mode='sum'))\n",
        "  \n",
        "  model.add(Dense(17, activation='sigmoid'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def GoogLeNet():\n",
        "  base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(247, 242, 3))\n",
        "\n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  # let's add a fully-connected layer\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  # and a logistic layer -- let's say we have 17 classes\n",
        "  predictions = Dense(17, activation='sigmoid')(x)\n",
        "\n",
        "  # this is the model we will train\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "  return model\n",
        "\n",
        "def ResNet50():\n",
        "  #base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(247, 242, 3))\n",
        "  base_model = applications.resnet50.ResNet50(weights= 'imagenet', include_top=False, input_shape= (247,242,3))\n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  # let's add a fully-connected layer\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  # and a logistic layer -- let's say we have 17 classes\n",
        "  predictions = Dense(17, activation='sigmoid')(x)\n",
        "\n",
        "  # this is the model we will train\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "  return model\n",
        "\n",
        "def residual_block(input, input_channels=None, output_channels=None, kernel_size=(3, 3), stride=1):\n",
        "\n",
        "    if output_channels is None:\n",
        "        output_channels = input.get_shape()[-1].value\n",
        "    if input_channels is None:\n",
        "        input_channels = output_channels // 4\n",
        "\n",
        "    strides = (stride, stride)\n",
        "\n",
        "    x = BatchNormalization()(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(input_channels, (1, 1))(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(input_channels, kernel_size, padding='same', strides=stride)(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(output_channels, (1, 1), padding='same')(x)\n",
        "\n",
        "    if input_channels != output_channels or stride != 1:\n",
        "        input = Conv2D(output_channels, (1, 1), padding='same', strides=strides)(input)\n",
        "\n",
        "    x = Add()([x, input])\n",
        "    return x\n",
        "  \n",
        "def attention_block(input, input_channels=None, output_channels=None, encoder_depth=1):\n",
        "\n",
        "    p = 1\n",
        "    t = 2\n",
        "    r = 1\n",
        "\n",
        "    if input_channels is None:\n",
        "        input_channels = input.get_shape()[-1].value\n",
        "    if output_channels is None:\n",
        "        output_channels = input_channels\n",
        "\n",
        "    # First Residual Block\n",
        "    for i in range(p):\n",
        "        input = residual_block(input)\n",
        "\n",
        "    # Trunc Branch\n",
        "    output_trunk = input\n",
        "    for i in range(t):\n",
        "        output_trunk = residual_block(output_trunk)\n",
        "\n",
        "    # Soft Mask Branch\n",
        "\n",
        "    ## encoder\n",
        "    ### first down sampling\n",
        "    output_soft_mask = MaxPool2D(padding='same')(input)  # 32x32\n",
        "    for i in range(r):\n",
        "        output_soft_mask = residual_block(output_soft_mask)\n",
        "\n",
        "    skip_connections = []\n",
        "    for i in range(encoder_depth - 1):\n",
        "\n",
        "        ## skip connections\n",
        "        output_skip_connection = residual_block(output_soft_mask)\n",
        "        skip_connections.append(output_skip_connection)\n",
        "        # print ('skip shape:', output_skip_connection.get_shape())\n",
        "\n",
        "        ## down sampling\n",
        "        output_soft_mask = MaxPool2D(padding='same')(output_soft_mask)\n",
        "        for _ in range(r):\n",
        "            output_soft_mask = residual_block(output_soft_mask)\n",
        "\n",
        "            ## decoder\n",
        "    skip_connections = list(reversed(skip_connections))\n",
        "    for i in range(encoder_depth - 1):\n",
        "        ## upsampling\n",
        "        for _ in range(r):\n",
        "            output_soft_mask = residual_block(output_soft_mask)\n",
        "        output_soft_mask = UpSampling2D()(output_soft_mask)\n",
        "        ## skip connections\n",
        "        output_soft_mask = Add()([output_soft_mask, skip_connections[i]])\n",
        "\n",
        "    ### last upsampling\n",
        "    for i in range(r):\n",
        "        output_soft_mask = residual_block(output_soft_mask)\n",
        "    output_soft_mask = UpSampling2D()(output_soft_mask)\n",
        "\n",
        "    ## Output\n",
        "    output_soft_mask = Conv2D(input_channels, (1, 1))(output_soft_mask)\n",
        "    output_soft_mask = Conv2D(input_channels, (1, 1))(output_soft_mask)\n",
        "    output_soft_mask = Activation('sigmoid')(output_soft_mask)\n",
        "\n",
        "    # Attention: (1 + output_soft_mask) * output_trunk\n",
        "    output = Lambda(lambda x: x + 1)(output_soft_mask)\n",
        "    output = Multiply()([output, output_trunk])  #\n",
        "\n",
        "    # Last Residual Block\n",
        "    for i in range(p):\n",
        "        output = residual_block(output)\n",
        "\n",
        "    return output\n",
        "  \n",
        "def ResAttentionNet56(shape=(256, 256, 3), n_channels=64, n_classes=17,\n",
        "                      dropout=0):\n",
        "\n",
        "\n",
        "    input_ = Input(shape=shape)\n",
        "    x = Conv2D(n_channels, (7, 7), strides=(2, 2), padding='same')(input_) # 112x112\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)  # 56x56\n",
        "\n",
        "    x = residual_block(x, output_channels=n_channels * 4)  # 56x56\n",
        "    x = attention_block(x, encoder_depth=3)  # bottleneck 7x7\n",
        "\n",
        "    x = residual_block(x, output_channels=n_channels * 8, stride=2)  # 28x28\n",
        "    x = attention_block(x, encoder_depth=2)  # bottleneck 7x7\n",
        "\n",
        "    x = residual_block(x, output_channels=n_channels * 16, stride=2)  # 14x14\n",
        "    x = attention_block(x, encoder_depth=1)  # bottleneck 7x7\n",
        "\n",
        "    x = residual_block(x, output_channels=n_channels * 32, stride=2)  # 7x7\n",
        "    x = residual_block(x, output_channels=n_channels * 32)\n",
        "    x = residual_block(x, output_channels=n_channels * 32)\n",
        "\n",
        "    pool_size = (x.get_shape()[1].value, x.get_shape()[2].value)\n",
        "    x = AveragePooling2D(pool_size=pool_size, strides=(1, 1))(x)\n",
        "    if dropout:\n",
        "        x = Dropout(dropout)(x)\n",
        "    output = Dense(n_classes, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(input_, output)\n",
        "    \n",
        "    return model\n",
        "\n",
        "def my_model(shape=(256,256,3)):\n",
        "  input_ = Input(shape=shape)\n",
        "  a1 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(input_)\n",
        "\n",
        "  x1 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(input_)\n",
        "  x1 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x1) \n",
        "  x1 = BatchNormalization()(x1)\n",
        "  x1 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x1)\n",
        "  x1 = concatenate([a1, x1])\n",
        "\n",
        "  a2 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(a1)\n",
        "  x2 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu')(x1)\n",
        "  x2 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu')(x2) \n",
        "  x2 = BatchNormalization()(x2)\n",
        "  x2 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x2)\n",
        "  x2 = concatenate([a2, x2])\n",
        "  \n",
        "  a3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(a2)\n",
        "  x3 = Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu')(x2)\n",
        "  x3 = Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu')(x3) \n",
        "  x3 = Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu')(x3) \n",
        "  x3 = BatchNormalization()(x3)\n",
        "  x3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x3)\n",
        "  x3 = concatenate([a3, x3])\n",
        "  \n",
        "  a4 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(a3)\n",
        "  x4 = Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu')(x3)\n",
        "  x4 = Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu')(x4) \n",
        "  x4 = Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu')(x4) \n",
        "  x4 = BatchNormalization()(x4)\n",
        "  x4 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x4)\n",
        "  x4 = concatenate([a4, x4])\n",
        "  \n",
        "  a5 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(a4)\n",
        "  x5 = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x4)\n",
        "  x5 = Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu')(x5) \n",
        "  x5 = Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu')(x5)\n",
        "  x5 = BatchNormalization()(x5)\n",
        "  x5 = Activation('relu')(x5)\n",
        "  x5 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x5)\n",
        "  x5 = concatenate([a5, x5])\n",
        "  \n",
        "  pool_size = (x5.get_shape()[1].value, x5.get_shape()[2].value)\n",
        "  x5 = AveragePooling2D(pool_size=pool_size, strides=(1, 1))(x5)\n",
        "  x5 = Flatten()(x5)\n",
        "  #x5 = Dropout(0.50)(x5)\n",
        "  \n",
        "  pool_size = (x4.get_shape()[1].value, x4.get_shape()[2].value)\n",
        "  x4 = AveragePooling2D(pool_size=pool_size, strides=(1, 1))(x4)\n",
        "  x4 = Flatten()(x4)\n",
        "  #x4 = Dropout(0.50)(x4)\n",
        "  \n",
        "  pool_size = (x3.get_shape()[1].value, x3.get_shape()[2].value)\n",
        "  x3 = AveragePooling2D(pool_size=pool_size, strides=(1, 1))(x3)\n",
        "  x3 = Flatten()(x3)\n",
        "  #x3 = Dropout(0.50)(x3)\n",
        "  \n",
        "  pool_size = (x2.get_shape()[1].value, x2.get_shape()[2].value)\n",
        "  x2 = AveragePooling2D(pool_size=pool_size, strides=(1, 1))(x2)\n",
        "  x2 = Flatten()(x2)\n",
        "  #x2 = Dropout(0.50)(x2)\n",
        "  \n",
        "  pool_size = (x1.get_shape()[1].value, x1.get_shape()[2].value)\n",
        "  x1 = AveragePooling2D(pool_size=pool_size, strides=(1, 1))(x1)\n",
        "  x1 = Flatten()(x1)\n",
        "  #x1 = Dropout(0.50)(x1)\n",
        "\n",
        "  x = concatenate([x1, x2, x3, x4, x5],axis=-1)\n",
        "\n",
        "  x = Dense(4096, activation='relu')(x)\n",
        "  x = Dense(4096, activation='relu')(x)\n",
        "\n",
        "  output = Dense(17, activation='sigmoid')(x)\n",
        "  \n",
        "  model = Model(input_, output)\n",
        "  \n",
        "    \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJJSHnxR3mKg",
        "colab_type": "code",
        "outputId": "f4442b12-4faa-47bf-ef7c-ddd7ca3d9f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model1 = my_model()\n",
        "op = Adam(lr=0.0001)\n",
        "model1.compile(loss='binary_crossentropy', optimizer=op, metrics=['accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 256, 256, 64) 1792        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 256, 256, 64) 256         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling2D) (None, 128, 128, 3)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling2D) (None, 128, 128, 64) 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 128, 128, 67) 0           max_pooling2d_21[0][0]           \n",
            "                                                                 max_pooling2d_22[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 128, 128, 128 77312       concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 128, 128, 128 147584      conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 128, 128, 128 512         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling2D) (None, 64, 64, 3)    0           max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling2D) (None, 64, 64, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 64, 64, 131)  0           max_pooling2d_23[0][0]           \n",
            "                                                                 max_pooling2d_24[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 64, 64, 256)  302080      concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 64, 64, 256)  1024        conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling2D) (None, 32, 32, 3)    0           max_pooling2d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling2D) (None, 32, 32, 256)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 32, 32, 259)  0           max_pooling2d_25[0][0]           \n",
            "                                                                 max_pooling2d_26[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 32, 32, 512)  1193984     concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 32, 32, 512)  2048        conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling2D) (None, 16, 16, 3)    0           max_pooling2d_25[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling2D) (None, 16, 16, 512)  0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 515)  0           max_pooling2d_27[0][0]           \n",
            "                                                                 max_pooling2d_28[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 512)  2373632     concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 512)  2048        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 512)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling2D) (None, 8, 8, 3)      0           max_pooling2d_27[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling2D) (None, 8, 8, 512)    0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 8, 8, 515)    0           max_pooling2d_29[0][0]           \n",
            "                                                                 max_pooling2d_30[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_15 (AveragePo (None, 1, 1, 67)     0           concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_14 (AveragePo (None, 1, 1, 131)    0           concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_13 (AveragePo (None, 1, 1, 259)    0           concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, 1, 1, 515)    0           concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_11 (AveragePo (None, 1, 1, 515)    0           concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_15 (Flatten)            (None, 67)           0           average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_14 (Flatten)            (None, 131)          0           average_pooling2d_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_13 (Flatten)            (None, 259)          0           average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 515)          0           average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 515)          0           average_pooling2d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 1487)         0           flatten_15[0][0]                 \n",
            "                                                                 flatten_14[0][0]                 \n",
            "                                                                 flatten_13[0][0]                 \n",
            "                                                                 flatten_12[0][0]                 \n",
            "                                                                 flatten_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 4096)         6094848     concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 4096)         16781312    dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 17)           69649       dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 37,704,401\n",
            "Trainable params: 37,701,457\n",
            "Non-trainable params: 2,944\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "011rY63P3qkj",
        "colab_type": "code",
        "outputId": "c18ad9ba-e132-451b-93ab-c1a173d12b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data_augmentation = 0\n",
        "if data_augmentation == 1:\n",
        "  model1.fit_generator( data_generator.flow(train_set, train_labels_encoded, batch_size=32),\n",
        "                                               epochs=35,\n",
        "                                               steps_per_epoch=1600 // 32,\n",
        "                                               verbose=2,\n",
        "                                               validation_data=(test_set,test_labels_encoded),\n",
        "                                               callbacks=CALLBACKS() )\n",
        "else:\n",
        "  model1.fit( train_set, train_labels, batch_size = 32, epochs = 35, \n",
        "                    validation_data=(test_set,test_labels), callbacks=CALLBACKS())\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
            "  warnings.warn('`epsilon` argument is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1700 samples, validate on 400 samples\n",
            "Epoch 1/35\n",
            "1700/1700 [==============================] - 57s 34ms/step - loss: 0.3007 - acc: 0.8663 - val_loss: 0.2901 - val_acc: 0.8797\n",
            "Epoch 2/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.2275 - acc: 0.8999 - val_loss: 0.3515 - val_acc: 0.8685\n",
            "Epoch 3/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.1976 - acc: 0.9143 - val_loss: 0.2861 - val_acc: 0.8772\n",
            "Epoch 4/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.1814 - acc: 0.9212 - val_loss: 0.2481 - val_acc: 0.8937\n",
            "Epoch 5/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.1698 - acc: 0.9263 - val_loss: 0.2446 - val_acc: 0.9041\n",
            "Epoch 6/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.1532 - acc: 0.9336 - val_loss: 0.1995 - val_acc: 0.9141\n",
            "Epoch 7/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.1373 - acc: 0.9412 - val_loss: 0.2018 - val_acc: 0.9138\n",
            "Epoch 8/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.1313 - acc: 0.9443 - val_loss: 0.2231 - val_acc: 0.9056\n",
            "Epoch 9/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.1170 - acc: 0.9490 - val_loss: 0.2420 - val_acc: 0.9060\n",
            "Epoch 10/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.1181 - acc: 0.9488 - val_loss: 0.2861 - val_acc: 0.9100\n",
            "Epoch 11/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.1156 - acc: 0.9514 - val_loss: 0.3346 - val_acc: 0.8801\n",
            "Epoch 12/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0982 - acc: 0.9590 - val_loss: 0.1784 - val_acc: 0.9269\n",
            "Epoch 13/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.1043 - acc: 0.9559 - val_loss: 0.2162 - val_acc: 0.9188\n",
            "Epoch 14/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0918 - acc: 0.9618 - val_loss: 0.2482 - val_acc: 0.8982\n",
            "Epoch 15/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0948 - acc: 0.9612 - val_loss: 0.1465 - val_acc: 0.9396\n",
            "Epoch 16/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0764 - acc: 0.9680 - val_loss: 0.1504 - val_acc: 0.9397\n",
            "Epoch 17/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0657 - acc: 0.9733 - val_loss: 0.1740 - val_acc: 0.9324\n",
            "Epoch 18/35\n",
            "1700/1700 [==============================] - 54s 32ms/step - loss: 0.0643 - acc: 0.9737 - val_loss: 0.2365 - val_acc: 0.9187\n",
            "Epoch 19/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0756 - acc: 0.9689 - val_loss: 0.2547 - val_acc: 0.9187\n",
            "Epoch 20/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0570 - acc: 0.9778 - val_loss: 0.1447 - val_acc: 0.9432\n",
            "Epoch 21/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0478 - acc: 0.9819 - val_loss: 0.1493 - val_acc: 0.9416\n",
            "Epoch 22/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0406 - acc: 0.9848 - val_loss: 0.1976 - val_acc: 0.9288\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "Epoch 23/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0283 - acc: 0.9911 - val_loss: 0.1315 - val_acc: 0.9526\n",
            "Epoch 24/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0207 - acc: 0.9939 - val_loss: 0.1251 - val_acc: 0.9543\n",
            "Epoch 25/35\n",
            "1700/1700 [==============================] - 54s 32ms/step - loss: 0.0195 - acc: 0.9948 - val_loss: 0.1242 - val_acc: 0.9543\n",
            "Epoch 26/35\n",
            "1700/1700 [==============================] - 54s 32ms/step - loss: 0.0181 - acc: 0.9955 - val_loss: 0.1238 - val_acc: 0.9554\n",
            "Epoch 27/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0165 - acc: 0.9962 - val_loss: 0.1223 - val_acc: 0.9537\n",
            "Epoch 28/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0171 - acc: 0.9960 - val_loss: 0.1282 - val_acc: 0.9531\n",
            "Epoch 29/35\n",
            "1700/1700 [==============================] - 54s 32ms/step - loss: 0.0156 - acc: 0.9968 - val_loss: 0.1275 - val_acc: 0.9541\n",
            "Epoch 30/35\n",
            "1700/1700 [==============================] - 54s 32ms/step - loss: 0.0146 - acc: 0.9966 - val_loss: 0.1300 - val_acc: 0.9549\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "Epoch 31/35\n",
            "1700/1700 [==============================] - 54s 32ms/step - loss: 0.0129 - acc: 0.9974 - val_loss: 0.1281 - val_acc: 0.9557\n",
            "Epoch 32/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0140 - acc: 0.9976 - val_loss: 0.1278 - val_acc: 0.9563\n",
            "Epoch 33/35\n",
            "1700/1700 [==============================] - 55s 32ms/step - loss: 0.0118 - acc: 0.9983 - val_loss: 0.1276 - val_acc: 0.9566\n",
            "Epoch 34/35\n",
            "1700/1700 [==============================] - 54s 32ms/step - loss: 0.0122 - acc: 0.9980 - val_loss: 0.1277 - val_acc: 0.9568\n",
            "Epoch 35/35\n",
            "1700/1700 [==============================] - 54s 32ms/step - loss: 0.0122 - acc: 0.9982 - val_loss: 0.1273 - val_acc: 0.9568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7j5MlWqcFLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "be3428c6-e90c-4e0a-a614-9059a0be581e"
      },
      "source": [
        "thresholding1(test_set, test_labels)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.27 0.43 0.59 0.44 0.48 0.1  0.1  0.21 0.55 0.12 0.56 0.71 0.1  0.4\n",
            " 0.31 0.45 0.31]\n",
            "0.04058823529411765\n",
            "Accuracy is :: 0.8528214285714286\n",
            "F1 Score is :: 0.9194991254436934\n",
            "F2 Score is :: 0.9149417841684517\n",
            "Precision row :: 0.9271964285714286\n",
            "Recall row :: 0.9119285714285713\n",
            "Precision column :: 0.9324684092493127\n",
            "Recall column :: 0.8987238920530579\n",
            "  Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       0.87      0.78      0.82       144\n",
            "           2       0.87      0.83      0.85       131\n",
            "           3       0.89      0.82      0.86       163\n",
            "           4       1.00      1.00      1.00        24\n",
            "           5       0.81      0.57      0.67        23\n",
            "           6       1.00      1.00      1.00        24\n",
            "           7       1.00      0.89      0.94        18\n",
            "           8       0.90      0.89      0.89       175\n",
            "           9       0.95      1.00      0.98        21\n",
            "          10       0.92      0.94      0.93       235\n",
            "          11       0.98      0.80      0.88        60\n",
            "          12       1.00      1.00      1.00        27\n",
            "          13       1.00      1.00      1.00        24\n",
            "          14       0.78      0.89      0.83        28\n",
            "          15       0.91      0.92      0.92       182\n",
            "          16       0.97      0.95      0.96        37\n",
            "\n",
            "   micro avg       0.91      0.88      0.89      1329\n",
            "   macro avg       0.93      0.90      0.91      1329\n",
            "weighted avg       0.91      0.88      0.89      1329\n",
            " samples avg       0.93      0.91      0.91      1329\n",
            " \n",
            "\n",
            "(0.8091286307053942, 0.9090909090909091, 0.8803611738148984, 0.8944954128440368, 0.9271964285714286, 0.9119285714285713, 0.9194991254436934, 0.9149417841684517, 0.9324684092493127, 0.8987238920530579, 0.8528214285714286)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}